<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML" async></script>

  
  <meta name="description" content="线性回归什么是线性回归什么是回归
回归问题是函数拟合问题，就是给定一些点的集合，用一个曲线或者方程去拟合，使得所有点都大致符合

什么是线性回归
拟合的是一条直线，那就是线性回归

实现基础的线性回归实现回归的本质拿到一个好方程，输入已知量(自变量)，就能通过这个方程预测未知量(我们想要的数据)
拟" />
  

  
  
  
  
  
  
  <title>机器学习之线性回归基础 | 巫师实习员</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="线性回归什么是线性回归什么是回归 回归问题是函数拟合问题，就是给定一些点的集合，用一个曲线或者方程去拟合，使得所有点都大致符合  什么是线性回归 拟合的是一条直线，那就是线性回归  实现基础的线性回归实现回归的本质拿到一个好方程，输入已知量(自变量)，就能通过这个方程预测未知量(我们想要的数据) 拟合的方程我们假设能够拟合的方程是:$$h_\theta(x) &#x3D; \theta_0 + \">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之线性回归基础">
<meta property="og:url" content="http://example.com/2024/10/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="巫师实习员">
<meta property="og:description" content="线性回归什么是线性回归什么是回归 回归问题是函数拟合问题，就是给定一些点的集合，用一个曲线或者方程去拟合，使得所有点都大致符合  什么是线性回归 拟合的是一条直线，那就是线性回归  实现基础的线性回归实现回归的本质拿到一个好方程，输入已知量(自变量)，就能通过这个方程预测未知量(我们想要的数据) 拟合的方程我们假设能够拟合的方程是:$$h_\theta(x) &#x3D; \theta_0 + \">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241028030501.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241026163100.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241026163114.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241027162552.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241027150630.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241027150856.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241028060744.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241027151409.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020241027151659.png">
<meta property="article:published_time" content="2024-10-28T04:00:00.000Z">
<meta property="article:modified_time" content="2024-10-31T13:58:10.515Z">
<meta property="article:author" content="potter_program">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="线性回归">
<meta property="article:tag" content="最小二乘法">
<meta property="article:tag" content="梯度下降">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/Pasted%20image%2020241028030501.png">
  
  
    <link rel="icon" href="favicon.webp">
  
  
<link rel="stylesheet" href="/css/style.css">

  

  
  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
<meta name="generator" content="Hexo 7.3.0"></head>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="巫师实习员" rel="home">巫师实习员</a>
      </h1>
      
        <h2 class="site-description hitokoto"></h2>
        <script type="text/javascript" src="https://v1.hitokoto.cn/?encode=js"></script>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">Archives</a></li>
                
                </ul>
            </div>
    </nav>
</header>

      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-机器学习之线性回归" class="post-机器学习之线性回归 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      机器学习之线性回归基础
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://example.com/2024/10/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" data-id="cm2s5pqa400004kww5ftu0xjz" class="leave-reply bdsharebuttonbox" data-cmd="more">Share</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h2><h3 id="什么是回归"><a href="#什么是回归" class="headerlink" title="什么是回归"></a>什么是回归</h3><blockquote>
<p>回归问题是函数拟合问题，就是给定一些点的集合，用一个曲线或者方程去拟合，使得所有点都大致符合</p>
</blockquote>
<h3 id="什么是线性回归-1"><a href="#什么是线性回归-1" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h3><blockquote>
<p>拟合的是一条直线，那就是线性回归</p>
</blockquote>
<h2 id="实现基础的线性回归"><a href="#实现基础的线性回归" class="headerlink" title="实现基础的线性回归"></a>实现基础的线性回归</h2><h3 id="实现回归的本质"><a href="#实现回归的本质" class="headerlink" title="实现回归的本质"></a>实现回归的本质</h3><p>拿到一个<em>好方程</em>，输入已知量(自变量)，就能通过这个方程预测未知量(我们想要的数据)</p>
<h3 id="拟合的方程"><a href="#拟合的方程" class="headerlink" title="拟合的方程"></a>拟合的方程</h3><p>我们假设能够拟合的方程是:<br>$$<br>h_\theta(x) &#x3D; \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_n x_n<br>$$<br>为了一般化这个方程，我们引入常量x0&#x3D;1<br>$$ h_\theta(x_{n\times1)} &#x3D; \sum_{i&#x3D;0}^{n} \theta_i \cdot x_i &#x3D;<br>\theta_{n\times1}^Tx_{1\times n}$$<br>要实现回归，就找到一群系数θ，也就是一个θ向量，也称作特征分量。</p>
<h3 id="拟合的好坏评价"><a href="#拟合的好坏评价" class="headerlink" title="拟合的好坏评价"></a>拟合的好坏评价</h3><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>这个函数用来衡量我们预测值和真实值之间的差距<br>$$ J(\theta_{n\times1}) &#x3D; \frac{1}{2}\sum_{i&#x3D;1}^m(h_\theta(x_{n_\times1})^{(i)} - y^{(i)})^2$$</p>
<blockquote>
<p>(预测值-真实值)的平方，然后对每一组训练数据进行累加<br>需要一提的是，1&#x2F;2不是必要的，只是为了简化推导（最小二乘法的思想）</p>
</blockquote>
<p>损失函数的值小，就表明我们的误差小，那么我们的问题就变成了求最小值。<br>求$${minJ\theta}$$</p>
<h3 id="拟合的算法"><a href="#拟合的算法" class="headerlink" title="拟合的算法"></a>拟合的算法</h3><h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><p>显然损失函数是一个关于θ的二次函数，二次函数的图像不外乎像这样：<br><br><img src="/images/Pasted image 20241028030501.png" style="zoom:200%;"/><br>导数为零的时候自然就是损失最小的时候<br>求损失函数关于θ的导数：<br>$$导数 &#x3D; 2{x^T_{n\times1}}(y_{n\times1} - x_{n\times1}\theta_{n\times1})$$<br>令导数为零，得到：<br>$$\theta_{n\times1} &#x3D; (x^T_{n\times1}x_{n\times1})^{-1}x^T_{n\times1}y_{n\times1}$$<br>也就是说代入这个公式，就能拿到合适的θ向量，也就达成了目标。<br>代码实现也不是很困难</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">standRegres</span>(<span class="params">xArr, yArr</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">    函数说明：计算回归系数theta  </span></span><br><span class="line"><span class="string">    :param xArr: x数据集  </span></span><br><span class="line"><span class="string">    :param yArr: y数据集  </span></span><br><span class="line"><span class="string">    :return: 无  </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    xMat = np.asmatrix(xArr)  </span><br><span class="line">    yMat = np.asmatrix(yArr).T  </span><br><span class="line">    xTx = xMat.T * xMat  </span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) == <span class="number">0.0</span>:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;矩阵为奇异矩阵，不能求逆&#x27;</span>)  </span><br><span class="line">        <span class="keyword">return</span>  </span><br><span class="line">    theta = xTx.I * (xMat.T * yMat)  </span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>

<p><em>但是我们应该发现，这个公式中含有逆矩阵，然而现实中往往数据不可逆，因此最小二乘法不能适用于所有模型，而且，我们希望模型是不断从数据样本中学到有用的东西，而不是一步求解。</em><br>我们应该找一种更普遍的方法，目光看向梯度下降。</p>
<h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><h5 id="理解思路"><a href="#理解思路" class="headerlink" title="理解思路"></a>理解思路</h5><p>直观说梯度下降法的话，就是：</p>
<blockquote>
<p>我站在高山上，我想用最短的时间下山，但是每次只能走一步。那么我需要做的就是环顾一周，找到一个最陡峭的方向，然后移动到那个点上；到新位置之后，重复刚才的动作。每次都选择最陡峭的方向走，那么很快就能下山。</p>
</blockquote>
<p>就像这样：<br><img src="/images/Pasted image 20241026163100.png" style="zoom:300%;"/><br>条条道路通罗马，但是从图像来看，不同起点到的最低点不一定一样<br>那是不是需要从所有地方出发，拿到所有情况呢？</p>
<p>答案是否定的<br>因为损失函数的图长这样：<br><img src="/images/Pasted image 20241026163114.png" style="zoom:200%;"/><br>不用担心，它就是一个碗而已。</p>
<p>既然行得通，我们就放心考虑思路<br>最小梯度法的整体思路就是：<br>    1. 对θ进行赋值，这个值随机，通常复值一个全零的向量<br>    2. 不停迭代，每次迭代都改变θ，使得损失函数按照梯度下降的方向进行减少</p>
<h5 id="具体落实"><a href="#具体落实" class="headerlink" title="具体落实"></a>具体落实</h5><p>确定了下山要一步一步走，具体怎么落实呢？<br>很简单：找一个方向，走一步</p>
<ul>
<li><p><strong>找一个方向</strong></p>
<blockquote>
<p>下山哪个方向才是最陡峭的呢？<br>  首先想到的是斜率，在我数学知识不多的印象中：斜率越大越陡峭<br>  通过斜率又不难想到导数，所以这定是与导数相关的。<br>  简而言之就是<em>这一步</em>由<em>上一步的基础</em>和<em>某个导数</em>确定$$这一步 &#x3D; 上一步 ？ 某导数$$<br>  推不下去了，就拿出结论吧：$$\theta_j &#x3D; \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_{n\times1})$$
  </p>
</blockquote>
</li>
<li><p><strong>走一步</strong></p>
<blockquote>
<p>一步走多远？<br>上面式子中的α称为学习率(learning rate)，直白的说就是每一步的步长。<br>    1.α太大可能错过最小值，最后不收敛<br>        想象一下一个人一步可以跨的很大，大到可以从这个山头跨到那个山头，那么这个人就永远不能下山，一直在两个山头反复横跳<br>    2.α太小又会迭代很多次，消耗资源<br>        想象一下小碎步下山<br>拿张图就明白了<br><img src="/images/Pasted image 20241027162552.png" style="zoom:200%;"/></p>
</blockquote>
</li>
</ul>
<p>下山的关键就是那个迭代公式！<br>$$\theta_j &#x3D; \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_{n\times1})$$<br>对损失函数 ∇J(θ) 关于参数 θ 的偏导数（梯度）是：<br>$$∇J(θ) &#x3D; \frac{\partial J(\theta)}{\partial\theta} &#x3D; \frac{2}{m} X^T(X\theta - y)$$</p>
<p>数学转换，得到：<br>$$θ &#x3D; θ - \alpha(\frac{2}{m} X^T(X\theta - y))$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一些示例数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加偏置项</span></span><br><span class="line">X_b = np.c_[np.ones((X.shape[<span class="number">0</span>], <span class="number">1</span>)), X]  <span class="comment"># 在X的前面加一列1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">alpha = <span class="number">0.1</span>  <span class="comment"># 学习率</span></span><br><span class="line">iterations = <span class="number">1000</span>  <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权重</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 随机初始化权重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降算法</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">    predictions = X_b.dot(theta)  <span class="comment"># 计算预测值</span></span><br><span class="line">    errors = predictions - y  <span class="comment"># 计算误差</span></span><br><span class="line">    gradients = (<span class="number">2</span> / <span class="built_in">len</span>(X_b)) * X_b.T.dot(errors)  <span class="comment"># 计算梯度</span></span><br><span class="line">    theta -= alpha * gradients  <span class="comment"># 更新权重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最终的权重</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终的权重:&quot;</span>, theta)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>学习率通过不断尝试，得到最佳</p>
<p>还值得一提的是：</p>
<h5 id="提高效率"><a href="#提高效率" class="headerlink" title="提高效率"></a>提高效率</h5><p>为什么缩放？<br>特征值的范围是有差距的，有的特征影响大，有的影响小<br>就像你要竞选主席，你是中国公民这一特征的影响就远远小于你政治能力地位这一特征<br><img src="/images/Pasted image 20241027150630.png" style="zoom:200%;"/></p>
<p>为了让机器在读取数据的时候感觉更“舒服”，训练起来效率更高，还需要进行特征的缩放<br><img src="/images/Pasted image 20241027150856.png" style="zoom:200%;"/></p>
<p>特征缩放的常用方法</p>
<ul>
<li>最大值缩放 <img src="/images/Pasted image 20241028060744.png" style="zoom:200%;"/></li>
<li>均值归一化<img src="/images/Pasted image 20241027151409.png" style="zoom:200%;"/></li>
<li>Z-score 标准化<img src="/images/Pasted image 20241027151659.png" style="zoom:200%;"/></li>
</ul>
<p><br><br><br><br><br>放一些练手的连接，怕自己忘记了<br><a href="https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Supervised%20Machine%20Learning%20Regression%20and%20Classification/week2/5.Week%202%20practice%20lab%20Linear%20regression/.ipynb_checkpoints/C1_W2_Linear_Regression%20-%20%E5%89%AF%E6%9C%AC-checkpoint.ipynb" target="_blank">使用一个变量实施线性回归，以预测餐厅特许经营的利润</a><br><a href="https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Supervised%20Machine%20Learning%20Regression%20and%20Classification/week2/work/C1_W2_Lab05_Sklearn_GD_Soln.ipynb" target="_blank">利用 scikit-learn 使用 Gradient Descent 实现线性回归</a><br><a href="https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Supervised%20Machine%20Learning%20Regression%20and%20Classification/week2/work/C1_W2_Lab06_Sklearn_Normal_Soln.ipynb" target="_blank">利用 scikit-learn 使用基于正态方程的紧密形式解实现线性回归</a><br><br><br><br><br>写在最后，感谢这些文章和视频对我的指点：<br><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/tohuangjia/linear-regression">https://www.kaggle.com/code/tohuangjia/linear-regression</a><br><a target="_blank" rel="noopener" href="https://cuijiahua.com/blog/2017/11/ml_11_regression_1.html">https://cuijiahua.com/blog/2017/11/ml_11_regression_1.html</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/didiaopao/article/details/126483324">https://blog.csdn.net/didiaopao/article/details/126483324</a><br><a target="_blank" rel="noopener" href="https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Unsupervised%20learning%20recommenders%20reinforcement%20learning/week1/2%20Practice%20Lab1/.ipynb_checkpoints/C3_W1_KMeans_Assignment-checkpoint.ipynb">https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Unsupervised%20learning%20recommenders%20reinforcement%20learning/week1/2%20Practice%20Lab1/.ipynb_checkpoints/C3_W1_KMeans_Assignment-checkpoint.ipynb</a><br><a target="_blank" rel="noopener" href="https://njuferret.github.io/2017/08/24/Gradient-Descent/">https://njuferret.github.io/2017/08/24/Gradient-Descent/</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/renly/archive/2013/01/04/2844880.html">https://www.cnblogs.com/renly/archive/2013/01/04/2844880.html</a><br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1PN4y1V7d9/?p=3&vd_source=696b6322fafa5344c9a7c99a18c22374">https://www.bilibili.com/video/BV1PN4y1V7d9/?p=3&amp;vd_source=696b6322fafa5344c9a7c99a18c22374</a><br><a target="_blank" rel="noopener" href="https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Supervised%20Machine%20Learning%20Regression%20and%20Classification/week1/6.Train%20the%20model%20with%20gradient%20descent/C1_W1_Lab05_Gradient_Descent_Soln.ipynb">https://github.com/kaieye/2022-Machine-Learning-Specialization/blob/main/Supervised%20Machine%20Learning%20Regression%20and%20Classification/week1/6.Train%20the%20model%20with%20gradient%20descent/C1_W1_Lab05_Gradient_Descent_Soln.ipynb</a></p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2024/10/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">
    <time datetime="2024-10-28T04:00:00.000Z" class="entry-date">
        2024-10-28
    </time>
</a>
    
    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" rel="tag">最小二乘法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">梯度下降</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2024/10/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="prev"><span class="meta-nav">←</span> 机器学习之逻辑回归基础</a></span>
    
    
        <span class="nav-next"><a href="/2024/10/20/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3(%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84)/" rel="next">滑动窗口leetcode209(长度最小的子数组) <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <!-- <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside> -->
<aside id="search" class="widget widget_search">
    <form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="/search.html">
        <div>
            <input type="text" value="" name="q" id="s" required placeholder="搜索文章" />
            <input type="submit" id="searchsubmit" value="搜索" />
        </div>
    </form>
</aside>


  
    
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2024/12/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BSVM%E5%9F%BA%E7%A1%80/">机器学习之SVM基础</a>
          </li>
        
          <li>
            <a href="/2024/11/04/K-Means++/">K-Means++</a>
          </li>
        
          <li>
            <a href="/2024/11/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">机器学习之聚类算法基础</a>
          </li>
        
          <li>
            <a href="/2024/11/01/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3(%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2)/">滑动窗口leetcode76(最小覆盖子串)</a>
          </li>
        
          <li>
            <a href="/2024/10/31/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3(%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE)/">滑动窗口leetcode904(水果成篮)</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-Means/" rel="tag">K-Means</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" rel="tag">二分查找</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%86%E7%B1%BB/" rel="tag">分类</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" rel="tag">双指针</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88/" rel="tag">快慢指针</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E7%BB%84/" rel="tag">数组</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" rel="tag">最小二乘法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" rel="tag">梯度下降</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" rel="tag">滑动窗口</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A2%8E%E7%AC%94/" rel="tag">碎笔</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/" rel="tag">移除元素</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">16</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%81%9A%E7%B1%BB/" rel="tag">聚类</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/K-Means/" style="font-size: 11.67px;">K-Means</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/leetcode/" style="font-size: 18.33px;">leetcode</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 15px;">二分查找</a> <a href="/tags/%E5%88%86%E7%B1%BB/" style="font-size: 10px;">分类</a> <a href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" style="font-size: 11.67px;">双指针</a> <a href="/tags/%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88/" style="font-size: 13.33px;">快慢指针</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 10px;">排序</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" style="font-size: 10px;">支持向量机</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size: 16.67px;">数组</a> <a href="/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" style="font-size: 10px;">最小二乘法</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 10px;">梯度下降</a> <a href="/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" style="font-size: 11.67px;">滑动窗口</a> <a href="/tags/%E7%A2%8E%E7%AC%94/" style="font-size: 10px;">碎笔</a> <a href="/tags/%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/" style="font-size: 15px;">移除元素</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 20px;">算法</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%81%9A%E7%B1%BB/" style="font-size: 11.67px;">聚类</a> <a href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">逻辑回归</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2024 potter_program
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
</footer>
    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/js/share.js'];</script>

<script src="/js/jquery-3.3.1.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
</body>
</html>